{
  "solution_code": "#include <cuda_runtime.h>\n\n__global__ void reference_matrix_multiply(float* A, float* B, float* C, size_t M, size_t N, size_t K) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        float sum = 0.0f;\n        for (int i = 0; i < K; i++) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\nextern \"C\" void solution(float* input_a, float* input_b, float* output_c, size_t m, size_t n, size_t k) {\n    dim3 blockDim(16, 16);\n    dim3 gridDim((n + blockDim.x - 1) / blockDim.x, \n                 (m + blockDim.y - 1) / blockDim.y);\n\n    reference_matrix_multiply<<<gridDim, blockDim>>>(input_a, input_b, output_c, m, n, k);   \n}",
  "problem": "matrix_multiplication",
  "problem_def": "import torch\nimport ctypes\nfrom typing import List, Dict, Tuple, Any\n\nfrom problem import Problem\n\n\nclass matrix_multiplication(Problem):\n    \"\"\"Matrix multiplication problem.\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"matrix-multiplication\"\n        )\n    \n    def reference_solution(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        PyTorch implementation of matrix multiplication.\n        \n        Args:\n            A: First input matrix\n            B: Second input matrix\n            \n        Returns:\n            Result of A * B\n        \"\"\"\n        with torch.no_grad():\n            return torch.matmul(A, B)\n    \n    def generate_test_cases(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate test cases for matrix multiplication.\n        \n        Returns:\n            List of test case dictionaries with varying matrix dimensions\n        \"\"\"\n        # Matrix dimensions: (M, K) Ã— (K, N) = (M, N)\n        # dims represents (M, N, K)\n        test_matrices = [\n            {\n                \"name\": \"4092x4092 x 4092x4092 matrices\",\n                \"dims\": (4092, 4092, 4092),\n            },\n            {\n                \"name\": \"8192x8192 x 8192x4092 matrices\",\n                \"dims\": (8192, 4092, 8192),\n            },\n            {\n                \"name\": \"4092x4092 x 4092x8192 matrices\",\n                \"dims\": (4092, 8192, 4092),\n            },\n            {\n                \"name\": \"8192x8192 x 8192x8192 matrices\",\n                \"dims\": (8192, 8192, 8192),\n            }\n        ]\n        \n        return [\n            {\n                \"name\": matrix[\"name\"],\n                \"dims\": matrix[\"dims\"],\n                \"create_inputs\": lambda m=matrix[\"dims\"]: (\n                    torch.rand(m[0], m[2], device=\"cuda\", dtype=torch.float32),\n                    torch.rand(m[2], m[1], device=\"cuda\", dtype=torch.float32)\n                )\n            }\n            for matrix in test_matrices\n        ]\n    \n    def verify_result(self, expected_output: torch.Tensor, \n                     actual_output: torch.Tensor) -> Tuple[bool, Dict[str, Any]]:\n        \"\"\"\n        Verify if the matrix multiplication result is correct.\n        \n        Args:\n            expected_output: Output from reference solution\n            actual_output: Output from submitted solution\n            \n        Returns:\n            Tuple of (is_correct, debug_info)\n        \"\"\"\n        is_close = torch.allclose(actual_output, expected_output, rtol=1e-4, atol=1e-4)\n        \n        debug_info = {}\n        if not is_close:\n            diff = actual_output - expected_output\n            max_diff = torch.max(torch.abs(diff)).item()\n            mean_diff = torch.mean(torch.abs(diff)).item()\n            \n            debug_info = {\n                \"max_difference\": max_diff,\n                \"mean_difference\": mean_diff\n            }\n        \n        return is_close, debug_info\n    \n    def get_function_signature(self) -> Dict[str, Any]:\n        \"\"\"\n        Get the function signature for the matrix multiplication solution.\n        \n        IMPORTANT: Comments are required. Outline the FLOPs calculation.\n        \n        Returns:\n            Dictionary with argtypes and restype for ctypes\n        \"\"\"\n        return {\n            \"argtypes\": [\n                ctypes.POINTER(ctypes.c_float),  # matrix_a\n                ctypes.POINTER(ctypes.c_float),  # matrix_b\n                ctypes.POINTER(ctypes.c_float),  # matrix_c (output)\n                ctypes.c_size_t,                 # M (rows in A and C)\n                ctypes.c_size_t,                 # N (columns in B and C)\n                ctypes.c_size_t                  # K (columns in A, rows in B)\n            ],\n            \"restype\": None\n        }\n    \n    def get_flops(self, test_case: Dict[str, Any]) -> int:\n        \"\"\"\n        Get the number of floating point operations for the problem.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        Returns:\n            Number of floating point operations\n        \"\"\"\n        # Matrix multiplication FLOPS = 2 * M * N * K\n        # (One multiply and one add for each cell in the result, done K times)\n        M, N, K = test_case[\"dims\"]\n        return 2 * M * N * K\n    \n    def get_extra_params(self, test_case: Dict[str, Any]) -> List[Any]:\n        \"\"\"\n        Get extra parameters to pass to the CUDA solution.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        Returns:\n            List containing the dimensions M, N, K\n        \"\"\"\n        M, N, K = test_case[\"dims\"]\n        return [M, N, K]",
  "gpu": "T4"
}
