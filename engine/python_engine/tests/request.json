{
  "solution_code": "#include <cuda_runtime.h>\n\nsize_t cdiv(size_t x, size_t y) {\n    return (x+y-1)/y;\n}\n\n__global__ void kernel(const float*__restrict__ A, const float*__restrict__ B, float* C, size_t H, size_t W, size_t Kh, size_t Kw) {\n    extern __shared__ float sB[];\n    for(size_t i = threadIdx.y * 32 + threadIdx.x; i < Kh * Kw; i += blockDim.y * blockDim.x) {\n        sB[i] = B[i];\n    }\n    __syncthreads();\n    size_t i = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t j = blockIdx.x * blockDim.x + threadIdx.x;\n    if(i < H && j < W) {\n        float acc = 0;\n        for(size_t k = 0; k < Kh; k++) {\n            if(i + k < (Kh - 1) / 2 || i + k - (Kh - 1) / 2 >= H) {\n                continue;\n            }\n            size_t ak = i + k - (Kh - 1) / 2;\n            for(size_t l = 0; l < Kw; l++) {\n                if(j + l < (Kw - 1) / 2 || j + l - (Kw - 1) / 2 >= W) {\n                    continue;\n                }\n                size_t al = j + l - (Kw - 1) / 2;\n                acc += A[ak * W + al] * sB[k * Kw + l];\n            }\n        }\n        C[i * W + j] = acc;\n    }\n}\n\n// Note: A, B, and C are all device pointers to float arrays\nextern \"C\" void solution(float* A, float* B, float* C, size_t H, size_t W, size_t Kh, size_t Kw) {\n    cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, Kh*Kw*4);\n    kernel<<<dim3(cdiv(H,32),cdiv(W,32)),dim3(32,32),Kh*Kw*4>>>(A, B, C, H, W, Kh, Kw);\n}",
  "problem": "conv-2d",
  "problem_def": "import torch\nimport ctypes\nfrom typing import List, Dict, Tuple, Any\n\nfrom problem import Problem\n\n\nclass conv_2d(Problem):\n    \"\"\"2D convolution problem.\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"conv-2d\"\n        )\n    \n    def reference_solution(self, input_image: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        PyTorch implementation of 2D convolution.\n        \n        Args:\n            input_image: Input image tensor of shape (H, W)\n            kernel: Convolution kernel tensor of shape (Kh, Kw)\n            \n        Returns:\n            Result of convolution with zero padding\n        \"\"\"\n        with torch.no_grad():\n            # Ensure kernel sizes are odd\n            assert kernel.size(0) % 2 == 1, \"Kernel height must be odd\"\n            assert kernel.size(1) % 2 == 1, \"Kernel width must be odd\"\n            \n            # Perform 2D convolution using PyTorch's built-in function\n            # Convert to shape expected by conv2d: [batch, channels, height, width]\n            input_reshaped = input_image.view(1, 1, input_image.size(0), input_image.size(1))\n            kernel_reshaped = kernel.view(1, 1, kernel.size(0), kernel.size(1))\n            \n            # Calculate padding size to maintain the same output size\n            padding_h = kernel.size(0) // 2\n            padding_w = kernel.size(1) // 2\n            \n            # Perform convolution\n            result = torch.nn.functional.conv2d(\n                input_reshaped, \n                kernel_reshaped, \n                padding=(padding_h, padding_w)\n            )\n            \n            # Reshape back to original dimensions\n            return result.view(input_image.size(0), input_image.size(1))\n    \n    def generate_test_cases(self, dtype: torch.dtype) -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate test cases for 2D convolution.\n        \n        Returns:\n            List of test case dictionaries with varying sizes\n        \"\"\"\n        test_configs = [\n            (512, 512, 3, 3),\n            (1024, 1024, 5, 5),\n            (2048, 2048, 7, 7),\n            (4096, 4096, 9, 9),\n            (8192, 8192, 11, 11),\n            (16384, 16384, 13, 13),\n            (1024, 1024, 31, 31),\n            (2048, 2048, 63, 63),\n            (4096, 4096, 127, 127)\n        ]\n        \n        return [\n            {\n                \"name\": f\"H={h}, W={w}, Kh={kh}, Kw={kw}\",\n                \"height\": h,\n                \"width\": w,\n                \"kernel_height\": kh,\n                \"kernel_width\": kw,\n                \"create_inputs\": lambda h=h, w=w, kh=kh, kw=kw: (\n                    torch.rand((h, w), device=\"cuda\", dtype=dtype) * 10.0 - 5.0,  # uniform [-5, 5]\n                    torch.rand((kh, kw), device=\"cuda\", dtype=dtype) * 2.0 - 1.0  # uniform [-1, 1]\n                )\n            }\n            for h, w, kh, kw in test_configs\n        ]\n    \n    def verify_result(self, expected_output: torch.Tensor, \n                     actual_output: torch.Tensor, dtype: torch.dtype) -> Tuple[bool, Dict[str, Any]]:\n        \"\"\"\n        Verify if the convolution result is correct.\n        \n        Args:\n            expected_output: Output from reference solution\n            actual_output: Output from submitted solution\n            \n        Returns:\n            Tuple of (is_correct, debug_info)\n        \"\"\"\n        is_close = torch.allclose(actual_output, expected_output, rtol=1e-5, atol=1e-5)\n        \n        debug_info = {}\n        if not is_close:\n            diff = actual_output - expected_output\n            max_diff = torch.max(torch.abs(diff)).item()\n            mean_diff = torch.mean(torch.abs(diff)).item()\n            \n            # Find indices of largest differences\n            flat_diff = diff.flatten()\n            _, top_indices = torch.topk(torch.abs(flat_diff), min(5, flat_diff.numel()))\n            \n            # Convert flat indices back to 2D coordinates\n            h, w = expected_output.shape\n            sample_diffs = {}\n            for i, idx in enumerate(top_indices):\n                row = idx.item() // w\n                col = idx.item() % w\n                sample_diffs[f\"pos_{i}_({row},{col})\"] = {\n                    \"expected\": expected_output[row, col].item(),\n                    \"actual\": actual_output[row, col].item(),\n                    \"diff\": diff[row, col].item()\n                }\n            \n            debug_info = {\n                \"max_difference\": max_diff,\n                \"mean_difference\": mean_diff,\n                \"sample_differences\": sample_diffs\n            }\n        \n        return is_close, debug_info\n    \n    def get_function_signature(self) -> Dict[str, Any]:\n        \"\"\"\n        Get the function signature for the 2D convolution solution.\n        \n        Returns:\n            Dictionary with argtypes and restype for ctypes\n        \"\"\"\n        return {\n            \"argtypes\": [\n                ctypes.POINTER(ctypes.c_float),  # input_image\n                ctypes.POINTER(ctypes.c_float),  # kernel\n                ctypes.POINTER(ctypes.c_float),  # output\n                ctypes.c_size_t,                 # height (H)\n                ctypes.c_size_t,                 # width (W)\n                ctypes.c_size_t,                 # kernel_height (Kh)\n                ctypes.c_size_t                  # kernel_width (Kw)\n            ],\n            \"restype\": None\n        }\n    \n    def get_flops(self, test_case: Dict[str, Any]) -> int:\n        \"\"\"\n        Get the number of floating point operations for the problem.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        IMPORTANT: Comments are required. Outline the FLOPs calculation.\n        \n        Returns:\n            Number of floating point operations\n        \"\"\"\n        # For 2D convolution, each output pixel requires Kh*Kw multiplications and Kh*Kw-1 additions\n        H = test_case[\"height\"]\n        W = test_case[\"width\"]\n        Kh = test_case[\"kernel_height\"]\n        Kw = test_case[\"kernel_width\"]\n        \n        # Total FLOPs for the entire image: H*W output pixels, each requiring:\n        # Kh*Kw multiplications + (Kh*Kw-1) additions = 2*Kh*Kw - 1 FLOPs\n        # Following the test case's flop calculation which uses 2*H*W*Kh*Kw\n        # This is slightly different from our detailed calculation but aligns with the test code\n        return 2 * H * W * Kh * Kw\n    \n    def get_extra_params(self, test_case: Dict[str, Any]) -> List[Any]:\n        \"\"\"\n        Get extra parameters to pass to the CUDA solution.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        Returns:\n            List containing the image height H, width W, kernel height Kh, and kernel width Kw\n        \"\"\"\n        H = test_case[\"height\"]\n        W = test_case[\"width\"]\n        Kh = test_case[\"kernel_height\"]\n        Kw = test_case[\"kernel_width\"]\n        return [H, W, Kh, Kw]",
  "gpu": "T4",
  "dtype": "float32"
}
