{
  "solution_code": "#include <cuda_runtime.h>\n\n__global__ void leaky_relu_kernel(const float* input, float* output, size_t n, size_t m, float alpha) {\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t idy = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    if (idx < n && idy < m) {\n        size_t index = idy * n + idx;\n        float val = input[index];\n        output[index] = val > 0.0f ? val : alpha * val;\n    }\n}\n\n// Note: input, output are all device pointers to float32 arrays\nextern \"C\" void solution(const float* input, float alpha, float* output, size_t n, size_t m) {    \n    dim3 block_size(16, 16);\n    dim3 num_blocks((n + block_size.x - 1) / block_size.x,\n                    (m + block_size.y - 1) / block_size.y);\n    \n    leaky_relu_kernel<<<num_blocks, block_size>>>(input, output, n, m, alpha);\n    cudaDeviceSynchronize();\n\n}",
  "problem": "leaky-relu",
  "problem_def": "import torch\nimport ctypes\nfrom typing import List, Dict, Tuple, Any\n\nfrom problem import Problem\n\n\nclass leaky_relu(Problem):\n    \"\"\"Leaky ReLU activation function problem.\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"leaky-relu\"\n        )\n    \n    def reference_solution(self, input_matrix: torch.Tensor, alpha: float) -> torch.Tensor:\n        \"\"\"\n        PyTorch implementation of Leaky ReLU.\n        \n        Args:\n            input_matrix: Input matrix of shape (M, N)\n            alpha: Slope for negative values\n            \n        Returns:\n            Result of Leaky ReLU activation\n        \"\"\"\n        with torch.no_grad():\n            return torch.nn.functional.leaky_relu(input_matrix, alpha)\n    \n    def generate_test_cases(self, dtype: torch.dtype) -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate test cases for Leaky ReLU.\n        \n        Returns:\n            List of test case dictionaries with varying sizes and alpha values\n        \"\"\"\n        # Test case configurations with specific matrix sizes and alpha values\n        matrix_sizes = [\n            (4096, 4096),\n            (6144, 4096)\n        ]\n        \n        alpha_values = [0.01, 0.05, 0.1, 0.2]\n        \n        test_cases = []\n        for m, n in matrix_sizes:\n            for alpha in alpha_values:\n                test_cases.append({\n                    \"name\": f\"{m}x{n}, alpha={alpha}\",\n                    \"rows\": m,\n                    \"cols\": n,\n                    \"alpha\": alpha,\n                    \"create_inputs\": lambda m=m, n=n, alpha=alpha: (\n                        torch.rand((m, n), device=\"cuda\", dtype=dtype) * 10.0 - 5.0,  # uniform [-5, 5]\n                        alpha\n                    )\n                })\n        \n        return test_cases\n    \n    def verify_result(self, expected_output: torch.Tensor, \n                     actual_output: torch.Tensor, dtype: torch.dtype) -> Tuple[bool, Dict[str, Any]]:\n        \"\"\"\n        Verify if the Leaky ReLU result is correct.\n        \n        Args:\n            expected_output: Output from reference solution\n            actual_output: Output from submitted solution\n            \n        Returns:\n            Tuple of (is_correct, debug_info)\n        \"\"\"\n        is_close = torch.allclose(actual_output, expected_output, rtol=1e-5, atol=1e-5)\n        \n        debug_info = {}\n        if not is_close:\n            diff = actual_output - expected_output\n            max_diff = torch.max(torch.abs(diff)).item()\n            mean_diff = torch.mean(torch.abs(diff)).item()\n            \n            # Find indices of largest differences\n            flat_diff = diff.flatten()\n            _, top_indices = torch.topk(torch.abs(flat_diff), min(5, flat_diff.numel()))\n            \n            # Convert flat indices back to 2D coordinates\n            m, n = expected_output.shape\n            sample_diffs = {}\n            for i, idx in enumerate(top_indices):\n                row = idx.item() // n\n                col = idx.item() % n\n                sample_diffs[f\"({row}, {col})\"] = {\n                    \"expected\": expected_output[row, col].item(),\n                    \"actual\": actual_output[row, col].item(),\n                    \"diff\": diff[row, col].item()\n                }\n                        \n            debug_info = {\n                \"max_difference\": max_diff,\n                \"mean_difference\": mean_diff,\n                \"sample_differences\": sample_diffs\n            }\n        \n        return is_close, debug_info\n    \n    def get_function_signature(self) -> Dict[str, Any]:\n        \"\"\"\n        Get the function signature for the Leaky ReLU solution.\n        \n        Returns:\n            Dictionary with argtypes and restype for ctypes\n        \"\"\"\n        return {\n            \"argtypes\": [\n                ctypes.POINTER(ctypes.c_float),  # input_matrix\n                ctypes.c_float,                  # alpha\n                ctypes.POINTER(ctypes.c_float),  # output_matrix\n                ctypes.c_size_t,                 # rows (M)\n                ctypes.c_size_t                  # columns (N)\n            ],\n            \"restype\": None\n        }\n    \n    def get_flops(self, test_case: Dict[str, Any]) -> int:\n        \"\"\"\n        Get the number of floating point operations for the problem.\n        \n        Args:\n            test_case: The test case dictionary\n\n        IMPORTANT: Comments are required. Outline the FLOPs calculation.\n            \n        Returns:\n            Number of floating point operations\n        \"\"\"\n        M = test_case[\"rows\"]\n        N = test_case[\"cols\"]\n        \n        # M*N FLOPs:\n        # - Each element requires 1 comparison and at most 1 multiplication\n        # - We count this as 1 FLOP per element as per the test case\n        return M * N\n    \n    def get_extra_params(self, test_case: Dict[str, Any]) -> List[Any]:\n        \"\"\"\n        Get extra parameters to pass to the CUDA solution.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        Returns:\n            List containing the rows M, columns N, and alpha value\n        \"\"\"\n        M = test_case[\"rows\"]\n        N = test_case[\"cols\"]\n        return [M, N]",
  "gpu": "T4",
  "dtype": "float32",
  "language": "cuda"
}
