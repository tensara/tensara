{
  "solution_code": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv1d_triton_kernel(A_ptr, B_ptr, C_ptr, N, K, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    start_idx = pid * BLOCK_SIZE\n    offset = (K - 1) // 2  # Center the kernel\n    \n    # Compute input offsets safely\n    a_offsets = start_idx + tl.arange(0, BLOCK_SIZE + K - 1) - offset\n    a_mask = (a_offsets >= 0) & (a_offsets < N)\n    a_vals = tl.load(A_ptr + a_offsets, mask=a_mask, other=0.0)\n    \n    # Load kernel weights\n    b_offsets = tl.arange(0, K)\n    b_vals = tl.load(B_ptr + b_offsets)\n\n    # Compute convolution for each output element in block\n    for pos in range(BLOCK_SIZE):\n        output_idx = start_idx + pos\n        if output_idx < N:\n            conv_sum = tl.sum(a_vals[pos:pos + K] * b_vals)\n            tl.store(C_ptr + output_idx, conv_sum)\n\ndef solution(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, N: int, K: int):\n    assert A.is_cuda and B.is_cuda and C.is_cuda\n    assert B.numel() == K\n    assert C.numel() == N\n    \n    BLOCK_SIZE = 128  # Tunable for optimal performance\n    grid = (triton.cdiv(N, BLOCK_SIZE),)\n    \n    conv1d_triton_kernel[grid](A, B, C, N, K, BLOCK_SIZE=BLOCK_SIZE)",
  "problem": "conv-1d",
  "problem_def": "import torch\nimport ctypes\nfrom typing import List, Dict, Tuple, Any\n\nfrom problem import Problem\n\n\nclass conv_1d(Problem):\n    \"\"\"1D convolution problem.\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"conv-1d\"\n        )\n    \n    def reference_solution(self, input_signal: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        PyTorch implementation of 1D convolution.\n        \n        Args:\n            input_signal: Input signal tensor of shape (N)\n            kernel: Convolution kernel tensor of shape (K)\n            \n        Returns:\n            Result of convolution with zero padding\n        \"\"\"\n        with torch.no_grad(), torch.autocast(\"cuda\", enabled=False, dtype=torch.float32):\n            # Ensure kernel size is odd\n            assert kernel.size(0) % 2 == 1, \"Kernel size must be odd\"\n            \n            # Perform 1D convolution using PyTorch's built-in function\n            # Convert to shape expected by conv1d: [batch, channels, length]\n            input_reshaped = input_signal.view(1, 1, -1)\n            kernel_reshaped = kernel.view(1, 1, -1)\n            \n            # Calculate padding size to maintain the same output size\n            padding = kernel.size(0) // 2\n            \n            # Perform convolution\n            result = torch.nn.functional.conv1d(\n                input_reshaped, \n                kernel_reshaped, \n                padding=padding\n            )\n            \n            # Reshape back to original dimensions\n            return result.view(-1)\n    \n    def generate_test_cases(self, dtype: torch.dtype) -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate test cases for 1D convolution.\n        \n        Returns:\n            List of test case dictionaries with varying sizes\n        \"\"\"\n        \n        test_configs = [\n            (65536, 8191),\n            (32768, 8191),\n            (131072, 8191),\n            (524288, 8191)\n        ]\n        \n        return [\n            {\n                \"name\": f\"N={signal_size}, K={kernel_size}\",\n                \"signal_size\": signal_size,\n                \"kernel_size\": kernel_size,\n                \"create_inputs\": lambda s=signal_size, k=kernel_size: (\n                    torch.rand(s, device=\"cuda\", dtype=dtype) * 10.0 - 5.0,\n                    torch.rand(k, device=\"cuda\", dtype=dtype) * 2.0 - 1.0\n                )\n            }\n            for signal_size, kernel_size in test_configs\n        ]\n    \n    def verify_result(self, expected_output: torch.Tensor, \n                     actual_output: torch.Tensor, dtype: torch.dtype) -> Tuple[bool, Dict[str, Any]]:\n        \"\"\"\n        Verify if the convolution result is correct.\n        \n        Args:\n            expected_output: Output from reference solution\n            actual_output: Output from submitted solution\n            \n        Returns:\n            Tuple of (is_correct, debug_info)\n        \"\"\"\n        is_close = torch.allclose(actual_output, expected_output, rtol=1e-5, atol=1e-5)\n        \n        debug_info = {}\n        if not is_close:\n            diff = actual_output - expected_output\n            max_diff = torch.max(torch.abs(diff)).item()\n            mean_diff = torch.mean(torch.abs(diff)).item()\n            \n            # Find indices of largest differences\n            _, top_indices = torch.topk(torch.abs(diff), min(5, diff.numel()))\n            \n            sample_diffs = {\n                f\"{idx.item()}\": {\n                    \"expected\": expected_output[idx].item(),\n                    \"actual\": actual_output[idx].item(),\n                    \"diff\": diff[idx].item()\n                }\n                for idx in top_indices\n            }\n            \n            debug_info = {\n                \"max_difference\": max_diff,\n                \"mean_difference\": mean_diff,\n                \"sample_differences\": sample_diffs\n            }\n        \n        return is_close, debug_info\n    \n    def get_function_signature(self) -> Dict[str, Any]:\n        \"\"\"\n        Get the function signature for the 1D convolution solution.\n        \n        Returns:\n            Dictionary with argtypes and restype for ctypes\n        \"\"\"\n        return {\n            \"argtypes\": [\n                ctypes.POINTER(ctypes.c_float),  # input_signal\n                ctypes.POINTER(ctypes.c_float),  # kernel\n                ctypes.POINTER(ctypes.c_float),  # output\n                ctypes.c_size_t,                 # signal_size (N)\n                ctypes.c_size_t                  # kernel_size (K)\n            ],\n            \"restype\": None\n        }\n    \n    def get_flops(self, test_case: Dict[str, Any]) -> int:\n        \"\"\"\n        Get the number of floating point operations for the problem.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        IMPORTANT: Comments are required. Outline the FLOPs calculation.\n        \n        Returns:\n            Number of floating point operations\n        \"\"\"\n        # For each output element, we do kernel_size multiplications and kernel_size-1 additions\n        # Each output element requires K multiplications and K-1 additions\n        N = test_case[\"signal_size\"]\n        K = test_case[\"kernel_size\"]\n        \n\n        # mult-adds count as 2 FLOPs\n        flops_per_element = 2 * K - 1  \n        \n        return N * flops_per_element\n    \n    def get_extra_params(self, test_case: Dict[str, Any]) -> List[Any]:\n        \"\"\"\n        Get extra parameters to pass to the CUDA solution.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        Returns:\n            List containing the signal size N and kernel size K\n        \"\"\"\n        N = test_case[\"signal_size\"]\n        K = test_case[\"kernel_size\"]\n        return [N, K]",
  "gpu": "T4",
  "dtype": "float32",
  "language": "python"
}
