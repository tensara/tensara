{
  "solution_code": "#include <cuda_runtime.h>\n\n__global__ void reference_gemm_relu_kernel(const float* A, const float* W, const float* b, float* C, \n                                         size_t B, size_t N, size_t M) {\n    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < B && col < M) {\n        float sum = 0.0f;\n        for (size_t k = 0; k < N; k++) {\n            sum -= A[row * N + k] * W[col * N + k];\n        }\n        sum += b[col];\n        C[row * M + col] = sum > 0.0f ? sum : 0.0f;\n    }\n}\n\nextern \"C\" void solution(const float* A, const float* W, const float* b, float* C,\n                                 size_t B, size_t N, size_t M) {\n    dim3 block_size(16, 16);\n    dim3 num_blocks((M + block_size.x - 1) / block_size.x,\n                    (B + block_size.y - 1) / block_size.y);\n\n    reference_gemm_relu_kernel<<<num_blocks, block_size>>>(A, W, b, C, B, N, M);\n    cudaDeviceSynchronize();\n}",
  "problem": "gemm-relu",
  "problem_def": "import torch\nimport ctypes\nfrom typing import List, Dict, Tuple, Any\n\nfrom problem import Problem\n\n\nclass gemm_relu(Problem):\n    \"\"\"GEMM with Bias and ReLU problem.\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"gemm-relu\"\n        )\n    \n    def reference_solution(self, input_matrix: torch.Tensor, weights: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        PyTorch implementation of GEMM with bias and ReLU.\n        \n        Args:\n            input_matrix: Input matrix of shape (B, N) (batch_size, input_features)\n            weights: Weight matrix of shape (M, N) (output_features, input_features)\n            bias: Bias vector of shape (M) (output_features)\n            \n        Returns:\n            Result of ReLU(input_matrix @ weights.T + bias)\n        \"\"\"\n\n        with torch.no_grad(), torch.autocast(\"cuda\", enabled=False, dtype=torch.float32):\n            # Matrix multiplication: (B, N) @ (N, M) -> (B, M)\n            result = torch.mm(input_matrix, weights.t())\n            # Add bias: (B, M) + (M) -> (B, M)\n            result = result + bias\n            # Apply ReLU activation\n            result = torch.relu(result)\n            \n            return result\n    \n    def generate_test_cases(self, dtype: torch.dtype) -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate test cases for GEMM with Bias and ReLU.\n        \n        Returns:\n            List of test case dictionaries with varying sizes\n        \"\"\"\n        \n        test_configs = [\n            (512, 6144, 1024),\n            (512, 8192, 1024),\n            (512, 8192, 2048),\n            (1024, 1024, 1024),\n            (1024, 4096, 1024),\n            (1024, 4096, 2048)\n        ]\n        \n        return [\n            {\n                \"name\": f\"B={batch_size}, N={in_features}, M={out_features}\",\n                \"batch_size\": batch_size,\n                \"in_features\": in_features,\n                \"out_features\": out_features,\n                \"create_inputs\": lambda b=batch_size, n=in_features, m=out_features: (\n                    torch.rand((b, n), device=\"cuda\", dtype=dtype) * 2 - 1,\n                    torch.rand((m, n), device=\"cuda\", dtype=dtype) * 2 - 1,  \n                    torch.rand((m), device=\"cuda\", dtype=dtype) * 2 - 1  \n                )\n            }\n            for batch_size, in_features, out_features in test_configs\n        ]\n    \n    def verify_result(self, expected_output: torch.Tensor, \n                     actual_output: torch.Tensor, dtype: torch.dtype) -> Tuple[bool, Dict[str, Any]]:\n        \"\"\"\n        Verify if the GEMM with Bias and ReLU result is correct.\n        \n        Args:\n            expected_output: Output from reference solution\n            actual_output: Output from submitted solution\n            \n        Returns:\n            Tuple of (is_correct, debug_info)\n        \"\"\"\n        is_close = torch.allclose(actual_output, expected_output, rtol=1e-4, atol=1e-3)\n        \n        debug_info = {}\n        if not is_close:\n            diff = actual_output - expected_output\n            max_diff = torch.max(torch.abs(diff)).item()\n            mean_diff = torch.mean(torch.abs(diff)).item()\n            \n            # Find indices of largest differences\n            flat_diff = diff.flatten()\n            _, top_indices = torch.topk(torch.abs(flat_diff), min(5, flat_diff.numel()))\n            \n            # Convert flat indices back to 2D coordinates\n            B, M = expected_output.shape\n            sample_diffs = {}\n            for i, idx in enumerate(top_indices):\n                row = idx.item() // M\n                col = idx.item() % M\n                sample_diffs[f\"pos_{i}_({row},{col})\"] = {\n                    \"expected\": expected_output[row, col].item(),\n                    \"actual\": actual_output[row, col].item(),\n                    \"diff\": diff[row, col].item()\n                }\n            \n            # Check for differences in activation pattern\n            expected_zeros = (expected_output == 0).sum().item()\n            actual_zeros = (actual_output == 0).sum().item()\n            \n            debug_info = {\n                \"max_difference\": max_diff,\n                \"mean_difference\": mean_diff,\n                \"expected_zeros\": expected_zeros,\n                \"actual_zeros\": actual_zeros,\n                \"sample_differences\": sample_diffs\n            }\n        \n        return is_close, debug_info\n    \n    def get_function_signature(self) -> Dict[str, Any]:\n        \"\"\"\n        Get the function signature for the GEMM with Bias and ReLU solution.\n        \n        Returns:\n            Dictionary with argtypes and restype for ctypes\n        \"\"\"\n        return {\n            \"argtypes\": [\n                ctypes.POINTER(ctypes.c_float),  # input_matrix\n                ctypes.POINTER(ctypes.c_float),  # weights\n                ctypes.POINTER(ctypes.c_float),  # bias\n                ctypes.POINTER(ctypes.c_float),  # output\n                ctypes.c_size_t,                 # batch_size (B)\n                ctypes.c_size_t,                 # input_features (N)\n                ctypes.c_size_t                  # output_features (M)\n            ],\n            \"restype\": None\n        }\n    \n    def get_flops(self, test_case: Dict[str, Any]) -> int:\n        \"\"\"\n        Get the number of floating point operations for the problem.\n\n        IMPORTANT: Comments are required. Outline the FLOPs calculation.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        Returns:\n            Number of floating point operations\n        \"\"\"\n        \n        B = test_case[\"batch_size\"]\n        N = test_case[\"in_features\"]\n        M = test_case[\"out_features\"]\n        \n        # 2*B*N*M FLOPs for matrix multiplication:\n        # - Each output element requires N MAD operations (2*N FLOPs)\n        # - There are B*M output elements\n        #\n        # B*M FLOPs for bias addition:\n        # - Each of the B*M output elements requires 1 addition\n        return 2 * B * N * M + B * M\n    \n    def get_extra_params(self, test_case: Dict[str, Any]) -> List[Any]:\n        \"\"\"\n        Get extra parameters to pass to the CUDA solution.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        Returns:\n            List containing the batch_size B, input_features N, and output_features M\n        \"\"\"\n        B = test_case[\"batch_size\"]\n        N = test_case[\"in_features\"]\n        M = test_case[\"out_features\"]\n        return [B, N, M]",
  "gpu": "T4",
  "dtype": "float32",
  "language": "cuda"
}
