{
  "solution_code": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_activation_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    ReLU activation function kernel\n\n    Computes the element-wise function: {relu}(x) = \\max(x, 0)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.maximum(1, x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef solution(input: torch.Tensor, output: torch.Tensor, n: int, m: int):\n    n_elements = n * m\n    grid = (triton.cdiv(n_elements, 1024),)\n    relu_activation_kernel[grid](input, output, n_elements, BLOCK_SIZE=1024)",
  "problem": "relu",
  "problem_def": "import torch\nimport ctypes\nfrom typing import List, Dict, Tuple, Any\n\nfrom problem import Problem\n\n\nclass relu(Problem):\n    \"\"\"ReLU activation function problem.\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"relu\"\n        )\n    \n    def reference_solution(self, input_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        PyTorch implementation of ReLU.\n        \n        Args:\n            input_matrix: Input matrix of shape (M, N)\n            \n        Returns:\n            Result of ReLU activation\n        \"\"\"\n        with torch.no_grad():\n            return torch.relu(input_matrix)\n    \n    def generate_test_cases(self, dtype: torch.dtype) -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate test cases for ReLU.\n        \n        Returns:\n            List of test case dictionaries with varying sizes\n        \"\"\"\n        # Test case configurations with specific matrix sizes\n        test_configs = [\n            (\"4096x4096\", 4096, 4096),\n            (\"6144x4096\", 6144, 4096),\n            (\"4096x7168\", 4096, 7168),\n            (\"4096x8192\", 4096, 8192),\n            (\"8192x8192\", 8192, 8192)\n        ]\n        \n        return [\n            {\n                \"name\": name,\n                \"rows\": m,\n                \"cols\": n,\n                \"create_inputs\": lambda m=m, n=n: (\n                    torch.rand((m, n), device=\"cuda\", dtype=dtype) * 10.0 - 5.0,  # uniform [-5, 5]\n                )\n            }\n            for name, m, n in test_configs\n        ]\n    \n    def verify_result(self, expected_output: torch.Tensor, \n                     actual_output: torch.Tensor, dtype: torch.dtype) -> Tuple[bool, Dict[str, Any]]:\n        \"\"\"\n        Verify if the ReLU result is correct.\n        \n        Args:\n            expected_output: Output from reference solution\n            actual_output: Output from submitted solution\n            \n        Returns:\n            Tuple of (is_correct, debug_info)\n        \"\"\"\n        is_close = torch.allclose(actual_output, expected_output, rtol=1e-5, atol=1e-5)\n        \n        debug_info = {}\n        if not is_close:\n            diff = actual_output - expected_output\n            max_diff = torch.max(torch.abs(diff)).item()\n            mean_diff = torch.mean(torch.abs(diff)).item()\n            \n            # Find indices of largest differences\n            flat_diff = diff.flatten()\n            _, top_indices = torch.topk(torch.abs(flat_diff), min(5, flat_diff.numel()))\n            \n            # Convert flat indices back to 2D coordinates\n            m, n = expected_output.shape\n            sample_diffs = {}\n            for i, idx in enumerate(top_indices):\n                row = idx.item() // n\n                col = idx.item() % n\n                sample_diffs[f\"pos_{i}_({row},{col})\"] = {\n                    \"expected\": expected_output[row, col].item(),\n                    \"actual\": actual_output[row, col].item(),\n                    \"diff\": diff[row, col].item()\n                }\n            \n            # Check for differences in activation pattern\n            expected_nonzeros = (expected_output > 0).sum().item()\n            actual_nonzeros = (actual_output > 0).sum().item()\n            \n            debug_info = {\n                \"max_difference\": max_diff,\n                \"mean_difference\": mean_diff,\n                \"expected_nonzeros\": expected_nonzeros,\n                \"actual_nonzeros\": actual_nonzeros,\n                \"sample_differences\": sample_diffs\n            }\n        \n        return is_close, debug_info\n    \n    def get_function_signature(self) -> Dict[str, Any]:\n        \"\"\"\n        Get the function signature for the ReLU solution.\n        \n        Returns:\n            Dictionary with argtypes and restype for ctypes\n        \"\"\"\n        return {\n            \"argtypes\": [\n                ctypes.POINTER(ctypes.c_float),  # input_matrix\n                ctypes.POINTER(ctypes.c_float),  # output_matrix\n                ctypes.c_size_t,                 # rows (M)\n                ctypes.c_size_t                  # columns (N)\n            ],\n            \"restype\": None\n        }\n    \n    def get_flops(self, test_case: Dict[str, Any]) -> int:\n        \"\"\"\n        Get the number of floating point operations for the problem.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        IMPORTANT: Comments are required. Outline the FLOPs calculation.\n        \n        Returns:\n            Number of floating point operations\n        \"\"\"\n        # Extract dimensions from test case\n        M = test_case[\"rows\"]\n        N = test_case[\"cols\"]\n        \n        # M*N FLOPs:\n        # - Each element requires 1 comparison operation\n        # - We count this as 1 FLOP per element as per the test case\n        return M * N\n    \n    def get_extra_params(self, test_case: Dict[str, Any]) -> List[Any]:\n        \"\"\"\n        Get extra parameters to pass to the CUDA solution.\n        \n        Args:\n            test_case: The test case dictionary\n            \n        Returns:\n            List containing the rows M and columns N\n        \"\"\"\n        M = test_case[\"rows\"]\n        N = test_case[\"cols\"]\n        return [M, N]",
  "gpu": "T4",
  "dtype": "float32",
  "language": "python"
}
